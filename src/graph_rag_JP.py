from typing import List
import os
from pdfminer.high_level import extract_pages
from pdfminer.layout import LAParams, LTTextContainer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.graphs import Neo4jGraph
from openai import OpenAI
import chainlit as cl
import re

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
NEO4J_URI = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')
NEO4J_USERNAME = os.environ.get('NEO4J_USERNAME', 'neo4j')
NEO4J_PASSWORD = os.environ.get('NEO4J_PASSWORD')
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

# Neo4jã‚°ãƒ©ãƒ•ã®åˆæœŸåŒ–
graph = Neo4jGraph(
    url=NEO4J_URI,
    username=NEO4J_USERNAME,
    password=NEO4J_PASSWORD
)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    length_function=len,
    separators=["ã€‚", "ã€", "\n", " "]
)

@cl.on_chat_start
async def on_chat_start():
    files = None

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾…ã¡
    while files is None:
        files = await cl.AskFileMessage(
            content="PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„",
            accept=["application/pdf"],
            max_size_mb=20,
            timeout=180,
        ).send()

    file = files[0]
    msg = cl.Message(content=f"`{file.name}`ã‚’å‡¦ç†ä¸­...")
    await msg.send()

    try:
        # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        documents = extract_text_from_pdf(file.path)
        
        if not documents:
            raise ValueError("PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")

        # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
        split_documents = text_splitter.split_documents(documents)
        
        # çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ä½œæˆ
        create_knowledge_graph(split_documents)
        
        msg.content = f"`{file.name}`ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚è³ªå•ã—ã¦ãã ã•ã„ï¼"
        await msg.update()

    except Exception as e:
        error_message = f"åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}"
        await cl.Message(content=error_message).send()

def extract_text_from_pdf(pdf_path: str) -> List[Document]:
    """æ—¥æœ¬èªç¸¦æ›¸ãPDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    documents = []
    laparams = LAParams(detect_vertical=True, all_texts=True)
    
    for page_num, page_layout in enumerate(extract_pages(pdf_path, laparams=laparams), start=1):
        page_text = ""
        for element in page_layout:
            if isinstance(element, LTTextContainer):
                page_text += element.get_text()
        
        if page_text.strip():
            metadata = {"page": page_num, "source": f"page_{page_num}"}
            doc = Document(page_content=page_text.strip(), metadata=metadata)
            documents.append(doc)
            
    return documents

def create_knowledge_graph(documents: List[Document]):
    """çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ä½œæˆï¼ˆNeo4j Auraå¯¾å¿œç‰ˆï¼‰"""
    try:
        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
        graph.query("MATCH (n) DETACH DELETE n")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ¼ãƒ‰ã®ä½œæˆ
        for doc in documents:
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
            graph.query("""
            CREATE (d:Document {
                text: $text,
                page: $page,
                source: $source
            })
            """, {
                "text": doc.page_content,
                "page": doc.metadata["page"],
                "source": doc.metadata["source"]
            })
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æŠ½å‡ºã¨é–¢é€£ä»˜ã‘
            keywords = extract_keywords(doc.page_content)
            for keyword in keywords:
                graph.query("""
                MERGE (k:Keyword {text: $keyword})
                WITH k
                MATCH (d:Document {page: $page})
                CREATE (d)-[:HAS_KEYWORD]->(k)
                """, {
                    "keyword": keyword,
                    "page": doc.metadata["page"]
                })
        
        # é€£ç¶šã™ã‚‹ãƒšãƒ¼ã‚¸é–“ã®é–¢ä¿‚ã‚’ä½œæˆ
        graph.query("""
        MATCH (d1:Document), (d2:Document)
        WHERE d1.page = d2.page - 1
        CREATE (d1)-[:NEXT]->(d2)
        """)
        
        print(f"çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—ã¾ã—ãŸ: {len(documents)}ãƒšãƒ¼ã‚¸")
        
    except Exception as e:
        print(f"çŸ¥è­˜ã‚°ãƒ©ãƒ•ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise

def extract_keywords(text: str) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
    try:
        # OpenAI APIã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        client = OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸»ã«åè©ã‚„å›ºæœ‰åè©ï¼‰ã‚’5ã¤æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¿”ã—ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": text}
            ],
            temperature=0
        )
        
        keywords = response.choices[0].message.content.split(',')
        return [k.strip() for k in keywords if k.strip()]
        
    except Exception as e:
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

@cl.on_message
async def main(message: cl.Message):
    try:
        # é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œç´¢
        relevant_docs = search_documents(message.content)
        if not relevant_docs:
            await cl.Message(content="é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚").send()
            return

        # å‚ç…§è¦ç´ ã®ä½œæˆ
        text_elements = []
        for doc in relevant_docs:
            page_num = doc.metadata["page"]
            relevance = doc.metadata["relevance"]
            context = doc.metadata["context"]
            
            # å‰å¾Œã®ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’å«ã‚€å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
            context_info = []
            if context["prev_page"]:
                context_info.append(f"å‰ã®ãƒšãƒ¼ã‚¸: {context['prev_page']}")
            if context["next_page"]:
                context_info.append(f"æ¬¡ã®ãƒšãƒ¼ã‚¸: {context['next_page']}")
            
            context_text = "\n".join(context_info)
            
            text_elements.append(
                cl.Text(
                    name=f"source_{page_num}",
                    content=f"""
ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num} (é–¢é€£åº¦: {relevance})
{context_text}
---
{doc.page_content}
                    """.strip(),
                    display="side"
                )
            )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        prompt = f"""
        ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ï¼š
        
        {context}
        
        è³ªå•: {message.content}
        """

        # å›ç­”ã®ç”Ÿæˆ
        client = OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0
        )
        
        answer = response.choices[0].message.content
        
        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã®è¿½åŠ 
        source_info = [f"p.{doc.metadata['page']}(é–¢é€£åº¦:{doc.metadata['relevance']})" 
                      for doc in relevant_docs]
        answer += f"\n\nå‚ç…§å…ƒ: {', '.join(source_info)}"
        
        await cl.Message(content=answer, elements=text_elements).send()

    except Exception as e:
        error_message = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        await cl.Message(content=error_message).send()

def search_documents(query: str, k: int = 3) -> List[Document]:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ï¼ˆNeo4j Auraå¯¾å¿œç‰ˆï¼‰"""
    try:
        # ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        query_keywords = extract_keywords(query)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢
        results = graph.query("""
        // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
        MATCH (d:Document)-[:HAS_KEYWORD]->(k:Keyword)
        WHERE k.text IN $keywords
        
        // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        WITH d, COUNT(DISTINCT k) as keyword_matches
        
        // å‰å¾Œã®ãƒšãƒ¼ã‚¸ã‚‚å«ã‚ã‚‹
        OPTIONAL MATCH (d)<-[:NEXT]-(prev:Document)
        OPTIONAL MATCH (d)-[:NEXT]->(next:Document)
        
        // çµæœã‚’è¿”ã™
        RETURN 
            d.text as text,
            d.page as page,
            d.source as source,
            keyword_matches as relevance,
            prev.page as prev_page,
            next.page as next_page
        ORDER BY keyword_matches DESC, d.page
        LIMIT $limit
        """, {
            "keywords": query_keywords,
            "limit": k
        })
        
        documents = []
        for r in results:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
            metadata = {
                "page": r["page"],
                "source": r["source"],
                "relevance": r["relevance"],
                "context": {
                    "prev_page": r["prev_page"],
                    "next_page": r["next_page"]
                }
            }
            
            doc = Document(
                page_content=r["text"],
                metadata=metadata
            )
            documents.append(doc)
            
            print(f"æ¤œç´¢çµæœ: ãƒšãƒ¼ã‚¸ {r['page']} (é–¢é€£åº¦: {r['relevance']})")
        
        return documents
        
    except Exception as e:
        print(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        raise
